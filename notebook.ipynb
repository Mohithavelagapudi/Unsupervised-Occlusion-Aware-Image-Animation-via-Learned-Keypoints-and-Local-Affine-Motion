{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DChOIzvdj2cz",
        "outputId": "7049bb2b-1de6-4f5e-f424-185281a00da4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting virtualenv\n",
            "  Downloading virtualenv-20.26.1-py3-none-any.whl (3.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting distlib<1,>=0.3.7 (from virtualenv)\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (3.13.4)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (4.2.1)\n",
            "Installing collected packages: distlib, virtualenv\n",
            "Successfully installed distlib-0.3.8 virtualenv-20.26.1\n"
          ]
        }
      ],
      "source": [
        "pip install virtualenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!virtualenv env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDBEegm1j65e",
        "outputId": "c24e82cf-1fca-4555-c6ff-5d790dfce46f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created virtual environment CPython3.10.12.final.0-64 in 1068ms\n",
            "  creator CPython3Posix(dest=/content/env, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: pip==24.0, setuptools==69.5.1, wheel==0.43.0\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Activate the virtual environment\n",
        "!source env/bin/activate"
      ],
      "metadata": {
        "id": "yBI1G6WvkC0Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP9d2LOjkf0c",
        "outputId": "085721e7-cefc-4ff2-9d69-75afbf4ea214"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting attrs==19.3.0 (from -r requirements.txt (line 1))\n",
            "  Downloading attrs-19.3.0-py2.py3-none-any.whl (39 kB)\n",
            "Collecting backcall==0.1.0 (from -r requirements.txt (line 2))\n",
            "  Downloading backcall-0.1.0.zip (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bleach==3.1.5 (from -r requirements.txt (line 3))\n",
            "  Downloading bleach-3.1.5-py2.py3-none-any.whl (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cffi==1.11.5 (from -r requirements.txt (line 4))\n",
            "  Downloading cffi-1.11.5.tar.gz (438 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.5/438.5 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cloudpickle==0.5.3 (from -r requirements.txt (line 5))\n",
            "  Downloading cloudpickle-0.5.3-py2.py3-none-any.whl (13 kB)\n",
            "Collecting colorama==0.4.3 (from -r requirements.txt (line 6))\n",
            "  Downloading colorama-0.4.3-py2.py3-none-any.whl (15 kB)\n",
            "Collecting cycler==0.10.0 (from -r requirements.txt (line 7))\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Collecting dask==0.18.2 (from -r requirements.txt (line 8))\n",
            "  Downloading dask-0.18.2-py2.py3-none-any.whl (645 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.2/645.2 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting decorator==4.3.0 (from -r requirements.txt (line 9))\n",
            "  Downloading decorator-4.3.0-py2.py3-none-any.whl (9.2 kB)\n",
            "Collecting defusedxml==0.6.0 (from -r requirements.txt (line 10))\n",
            "  Downloading defusedxml-0.6.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting entrypoints==0.3 (from -r requirements.txt (line 11))\n",
            "  Downloading entrypoints-0.3-py2.py3-none-any.whl (11 kB)\n",
            "Collecting imageio==2.3.0 (from -r requirements.txt (line 12))\n",
            "  Downloading imageio-2.3.0-py2.py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-metadata==1.6.0 (from -r requirements.txt (line 13))\n",
            "  Downloading importlib_metadata-1.6.0-py2.py3-none-any.whl (30 kB)\n",
            "Collecting ipykernel==5.2.1 (from -r requirements.txt (line 14))\n",
            "  Downloading ipykernel-5.2.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.2/118.2 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipython==7.14.0 (from -r requirements.txt (line 15))\n",
            "  Downloading ipython-7.14.0-py3-none-any.whl (782 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.3/782.3 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (0.2.0)\n",
            "Collecting jedi==0.17.0 (from -r requirements.txt (line 17))\n",
            "  Downloading jedi-0.17.0-py2.py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Jinja2==2.11.2 (from -r requirements.txt (line 18))\n",
            "  Downloading Jinja2-2.11.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.8/125.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonschema==3.2.0 (from -r requirements.txt (line 19))\n",
            "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter-client==6.1.3 (from -r requirements.txt (line 20))\n",
            "  Downloading jupyter_client-6.1.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter-core==4.6.3 (from -r requirements.txt (line 21))\n",
            "  Downloading jupyter_core-4.6.3-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.3/83.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver==1.0.1 (from -r requirements.txt (line 22))\n",
            "  Downloading kiwisolver-1.0.1.tar.gz (31 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting MarkupSafe==1.1.1 (from -r requirements.txt (line 23))\n",
            "  Downloading MarkupSafe-1.1.1.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting matplotlib==2.2.2 (from -r requirements.txt (line 24))\n",
            "  Downloading matplotlib-2.2.2.tar.gz (37.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: mistune==0.8.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 25)) (0.8.4)\n",
            "Collecting nbconvert==5.6.1 (from -r requirements.txt (line 26))\n",
            "  Downloading nbconvert-5.6.1-py2.py3-none-any.whl (455 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.1/455.1 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nbformat==5.0.6 (from -r requirements.txt (line 27))\n",
            "  Downloading nbformat-5.0.6-py3-none-any.whl (170 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.6/170.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting networkx==2.1 (from -r requirements.txt (line 28))\n",
            "  Downloading networkx-2.1.zip (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting notebook==6.0.3 (from -r requirements.txt (line 29))\n",
            "  Downloading notebook-6.0.3-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.15.0 (from -r requirements.txt (line 30))\n",
            "  Downloading numpy-1.15.0.zip (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement opencv-python==4.2.0.34 (from versions: 3.4.0.14, 3.4.10.37, 3.4.11.39, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.15.55, 3.4.16.57, 3.4.16.59, 3.4.17.61, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.3.56, 4.5.4.58, 4.5.4.60, 4.5.5.62, 4.5.5.64, 4.6.0.66, 4.7.0.68, 4.7.0.72, 4.8.0.74, 4.8.0.76, 4.8.1.78, 4.9.0.80)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for opencv-python==4.2.0.34\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i-rc4ezkkkf",
        "outputId": "071933a0-0170-4e26-97b0-ff22859ca0a9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aI_xrvqKlTmy",
        "outputId": "701f2bcf-b193-41c9-9b5a-04408cc872b8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Code from https://github.com/hassony2/torch_videovision\n",
        "\"\"\"\n",
        "\n",
        "import numbers\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "from skimage.transform import resize, rotate\n",
        "from numpy.lib.arraypad import pad\n",
        "import torchvision\n",
        "\n",
        "import warnings\n",
        "\n",
        "from skimage import img_as_ubyte, img_as_float\n",
        "\n",
        "\n",
        "def crop_clip(clip, min_h, min_w, h, w):\n",
        "    if isinstance(clip[0], np.ndarray):\n",
        "        cropped = [img[min_h:min_h + h, min_w:min_w + w, :] for img in clip]\n",
        "\n",
        "    elif isinstance(clip[0], PIL.Image.Image):\n",
        "        cropped = [\n",
        "            img.crop((min_w, min_h, min_w + w, min_h + h)) for img in clip\n",
        "            ]\n",
        "    else:\n",
        "        raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
        "                        'but got list of {0}'.format(type(clip[0])))\n",
        "    return cropped\n",
        "\n",
        "\n",
        "def pad_clip(clip, h, w):\n",
        "    im_h, im_w = clip[0].shape[:2]\n",
        "    pad_h = (0, 0) if h < im_h else ((h - im_h) // 2, (h - im_h + 1) // 2)\n",
        "    pad_w = (0, 0) if w < im_w else ((w - im_w) // 2, (w - im_w + 1) // 2)\n",
        "\n",
        "    return pad(clip, ((0, 0), pad_h, pad_w, (0, 0)), mode='edge')\n",
        "\n",
        "\n",
        "def resize_clip(clip, size, interpolation='bilinear'):\n",
        "    if isinstance(clip[0], np.ndarray):\n",
        "        if isinstance(size, numbers.Number):\n",
        "            im_h, im_w, im_c = clip[0].shape\n",
        "            # Min spatial dim already matches minimal size\n",
        "            if (im_w <= im_h and im_w == size) or (im_h <= im_w\n",
        "                                                   and im_h == size):\n",
        "                return clip\n",
        "            new_h, new_w = get_resize_sizes(im_h, im_w, size)\n",
        "            size = (new_w, new_h)\n",
        "        else:\n",
        "            size = size[1], size[0]\n",
        "\n",
        "        scaled = [\n",
        "            resize(img, size, order=1 if interpolation == 'bilinear' else 0, preserve_range=True,\n",
        "                   mode='constant', anti_aliasing=True) for img in clip\n",
        "            ]\n",
        "    elif isinstance(clip[0], PIL.Image.Image):\n",
        "        if isinstance(size, numbers.Number):\n",
        "            im_w, im_h = clip[0].size\n",
        "            # Min spatial dim already matches minimal size\n",
        "            if (im_w <= im_h and im_w == size) or (im_h <= im_w\n",
        "                                                   and im_h == size):\n",
        "                return clip\n",
        "            new_h, new_w = get_resize_sizes(im_h, im_w, size)\n",
        "            size = (new_w, new_h)\n",
        "        else:\n",
        "            size = size[1], size[0]\n",
        "        if interpolation == 'bilinear':\n",
        "            pil_inter = PIL.Image.NEAREST\n",
        "        else:\n",
        "            pil_inter = PIL.Image.BILINEAR\n",
        "        scaled = [img.resize(size, pil_inter) for img in clip]\n",
        "    else:\n",
        "        raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
        "                        'but got list of {0}'.format(type(clip[0])))\n",
        "    return scaled\n",
        "\n",
        "\n",
        "def get_resize_sizes(im_h, im_w, size):\n",
        "    if im_w < im_h:\n",
        "        ow = size\n",
        "        oh = int(size * im_h / im_w)\n",
        "    else:\n",
        "        oh = size\n",
        "        ow = int(size * im_w / im_h)\n",
        "    return oh, ow\n",
        "\n",
        "\n",
        "class RandomFlip(object):\n",
        "    def __init__(self, time_flip=False, horizontal_flip=False):\n",
        "        self.time_flip = time_flip\n",
        "        self.horizontal_flip = horizontal_flip\n",
        "\n",
        "    def __call__(self, clip):\n",
        "        if random.random() < 0.5 and self.time_flip:\n",
        "            return clip[::-1]\n",
        "        if random.random() < 0.5 and self.horizontal_flip:\n",
        "            return [np.fliplr(img) for img in clip]\n",
        "\n",
        "        return clip\n",
        "\n",
        "\n",
        "class RandomResize(object):\n",
        "    \"\"\"Resizes a list of (H x W x C) numpy.ndarray to the final size\n",
        "    The larger the original image is, the more times it takes to\n",
        "    interpolate\n",
        "    Args:\n",
        "    interpolation (str): Can be one of 'nearest', 'bilinear'\n",
        "    defaults to nearest\n",
        "    size (tuple): (widht, height)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ratio=(3. / 4., 4. / 3.), interpolation='nearest'):\n",
        "        self.ratio = ratio\n",
        "        self.interpolation = interpolation\n",
        "\n",
        "    def __call__(self, clip):\n",
        "        scaling_factor = random.uniform(self.ratio[0], self.ratio[1])\n",
        "\n",
        "        if isinstance(clip[0], np.ndarray):\n",
        "            im_h, im_w, im_c = clip[0].shape\n",
        "        elif isinstance(clip[0], PIL.Image.Image):\n",
        "            im_w, im_h = clip[0].size\n",
        "\n",
        "        new_w = int(im_w * scaling_factor)\n",
        "        new_h = int(im_h * scaling_factor)\n",
        "        new_size = (new_w, new_h)\n",
        "        resized = resize_clip(\n",
        "            clip, new_size, interpolation=self.interpolation)\n",
        "\n",
        "        return resized\n",
        "\n",
        "\n",
        "class RandomCrop(object):\n",
        "    \"\"\"Extract random crop at the same location for a list of videos\n",
        "    Args:\n",
        "    size (sequence or int): Desired output size for the\n",
        "    crop in format (h, w)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        if isinstance(size, numbers.Number):\n",
        "            size = (size, size)\n",
        "\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, clip):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        img (PIL.Image or numpy.ndarray): List of videos to be cropped\n",
        "        in format (h, w, c) in numpy.ndarray\n",
        "        Returns:\n",
        "        PIL.Image or numpy.ndarray: Cropped list of videos\n",
        "        \"\"\"\n",
        "        h, w = self.size\n",
        "        if isinstance(clip[0], np.ndarray):\n",
        "            im_h, im_w, im_c = clip[0].shape\n",
        "        elif isinstance(clip[0], PIL.Image.Image):\n",
        "            im_w, im_h = clip[0].size\n",
        "        else:\n",
        "            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
        "                            'but got list of {0}'.format(type(clip[0])))\n",
        "\n",
        "        clip = pad_clip(clip, h, w)\n",
        "        im_h, im_w = clip.shape[1:3]\n",
        "        x1 = 0 if h == im_h else random.randint(0, im_w - w)\n",
        "        y1 = 0 if w == im_w else random.randint(0, im_h - h)\n",
        "        cropped = crop_clip(clip, y1, x1, h, w)\n",
        "\n",
        "        return cropped\n",
        "\n",
        "\n",
        "class RandomRotation(object):\n",
        "    \"\"\"Rotate entire clip randomly by a random angle within\n",
        "    given bounds\n",
        "    Args:\n",
        "    degrees (sequence or int): Range of degrees to select from\n",
        "    If degrees is a number instead of sequence like (min, max),\n",
        "    the range of degrees, will be (-degrees, +degrees).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, degrees):\n",
        "        if isinstance(degrees, numbers.Number):\n",
        "            if degrees < 0:\n",
        "                raise ValueError('If degrees is a single number,'\n",
        "                                 'must be positive')\n",
        "            degrees = (-degrees, degrees)\n",
        "        else:\n",
        "            if len(degrees) != 2:\n",
        "                raise ValueError('If degrees is a sequence,'\n",
        "                                 'it must be of len 2.')\n",
        "\n",
        "        self.degrees = degrees\n",
        "\n",
        "    def __call__(self, clip):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        img (PIL.Image or numpy.ndarray): List of videos to be cropped\n",
        "        in format (h, w, c) in numpy.ndarray\n",
        "        Returns:\n",
        "        PIL.Image or numpy.ndarray: Cropped list of videos\n",
        "        \"\"\"\n",
        "        angle = random.uniform(self.degrees[0], self.degrees[1])\n",
        "        if isinstance(clip[0], np.ndarray):\n",
        "            rotated = [rotate(image=img, angle=angle, preserve_range=True) for img in clip]\n",
        "        elif isinstance(clip[0], PIL.Image.Image):\n",
        "            rotated = [img.rotate(angle) for img in clip]\n",
        "        else:\n",
        "            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
        "                            'but got list of {0}'.format(type(clip[0])))\n",
        "\n",
        "        return rotated\n",
        "\n",
        "\n",
        "class ColorJitter(object):\n",
        "    \"\"\"Randomly change the brightness, contrast and saturation and hue of the clip\n",
        "    Args:\n",
        "    brightness (float): How much to jitter brightness. brightness_factor\n",
        "    is chosen uniformly from [max(0, 1 - brightness), 1 + brightness].\n",
        "    contrast (float): How much to jitter contrast. contrast_factor\n",
        "    is chosen uniformly from [max(0, 1 - contrast), 1 + contrast].\n",
        "    saturation (float): How much to jitter saturation. saturation_factor\n",
        "    is chosen uniformly from [max(0, 1 - saturation), 1 + saturation].\n",
        "    hue(float): How much to jitter hue. hue_factor is chosen uniformly from\n",
        "    [-hue, hue]. Should be >=0 and <= 0.5.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
        "        self.brightness = brightness\n",
        "        self.contrast = contrast\n",
        "        self.saturation = saturation\n",
        "        self.hue = hue\n",
        "\n",
        "    def get_params(self, brightness, contrast, saturation, hue):\n",
        "        if brightness > 0:\n",
        "            brightness_factor = random.uniform(\n",
        "                max(0, 1 - brightness), 1 + brightness)\n",
        "        else:\n",
        "            brightness_factor = None\n",
        "\n",
        "        if contrast > 0:\n",
        "            contrast_factor = random.uniform(\n",
        "                max(0, 1 - contrast), 1 + contrast)\n",
        "        else:\n",
        "            contrast_factor = None\n",
        "\n",
        "        if saturation > 0:\n",
        "            saturation_factor = random.uniform(\n",
        "                max(0, 1 - saturation), 1 + saturation)\n",
        "        else:\n",
        "            saturation_factor = None\n",
        "\n",
        "        if hue > 0:\n",
        "            hue_factor = random.uniform(-hue, hue)\n",
        "        else:\n",
        "            hue_factor = None\n",
        "        return brightness_factor, contrast_factor, saturation_factor, hue_factor\n",
        "\n",
        "    def __call__(self, clip):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        clip (list): list of PIL.Image\n",
        "        Returns:\n",
        "        list PIL.Image : list of transformed PIL.Image\n",
        "        \"\"\"\n",
        "        if isinstance(clip[0], np.ndarray):\n",
        "            brightness, contrast, saturation, hue = self.get_params(\n",
        "                self.brightness, self.contrast, self.saturation, self.hue)\n",
        "\n",
        "            # Create img transform function sequence\n",
        "            img_transforms = []\n",
        "            if brightness is not None:\n",
        "                img_transforms.append(lambda img: torchvision.transforms.functional.adjust_brightness(img, brightness))\n",
        "            if saturation is not None:\n",
        "                img_transforms.append(lambda img: torchvision.transforms.functional.adjust_saturation(img, saturation))\n",
        "            if hue is not None:\n",
        "                img_transforms.append(lambda img: torchvision.transforms.functional.adjust_hue(img, hue))\n",
        "            if contrast is not None:\n",
        "                img_transforms.append(lambda img: torchvision.transforms.functional.adjust_contrast(img, contrast))\n",
        "            random.shuffle(img_transforms)\n",
        "            img_transforms = [img_as_ubyte, torchvision.transforms.ToPILImage()] + img_transforms + [np.array,\n",
        "                                                                                                     img_as_float]\n",
        "\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.simplefilter(\"ignore\")\n",
        "                jittered_clip = []\n",
        "                for img in clip:\n",
        "                    jittered_img = img\n",
        "                    for func in img_transforms:\n",
        "                        jittered_img = func(jittered_img)\n",
        "                    jittered_clip.append(jittered_img.astype('float32'))\n",
        "        elif isinstance(clip[0], PIL.Image.Image):\n",
        "            brightness, contrast, saturation, hue = self.get_params(\n",
        "                self.brightness, self.contrast, self.saturation, self.hue)\n",
        "\n",
        "            # Create img transform function sequence\n",
        "            img_transforms = []\n",
        "            if brightness is not None:\n",
        "                img_transforms.append(lambda img: torchvision.transforms.functional.adjust_brightness(img, brightness))\n",
        "            if saturation is not None:\n",
        "                img_transforms.append(lambda img: torchvision.transforms.functional.adjust_saturation(img, saturation))\n",
        "            if hue is not None:\n",
        "                img_transforms.append(lambda img: torchvision.transforms.functional.adjust_hue(img, hue))\n",
        "            if contrast is not None:\n",
        "                img_transforms.append(lambda img: torchvision.transforms.functional.adjust_contrast(img, contrast))\n",
        "            random.shuffle(img_transforms)\n",
        "\n",
        "            # Apply to all videos\n",
        "            jittered_clip = []\n",
        "            for img in clip:\n",
        "                for func in img_transforms:\n",
        "                    jittered_img = func(img)\n",
        "                jittered_clip.append(jittered_img)\n",
        "\n",
        "        else:\n",
        "            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
        "                            'but got list of {0}'.format(type(clip[0])))\n",
        "        return jittered_clip\n",
        "\n",
        "\n",
        "class AllAugmentationTransform:\n",
        "    def __init__(self, resize_param=None, rotation_param=None, flip_param=None, crop_param=None, jitter_param=None):\n",
        "        self.transforms = []\n",
        "\n",
        "        if flip_param is not None:\n",
        "            self.transforms.append(RandomFlip(**flip_param))\n",
        "\n",
        "        if rotation_param is not None:\n",
        "            self.transforms.append(RandomRotation(**rotation_param))\n",
        "\n",
        "        if resize_param is not None:\n",
        "            self.transforms.append(RandomResize(**resize_param))\n",
        "\n",
        "        if crop_param is not None:\n",
        "            self.transforms.append(RandomCrop(**crop_param))\n",
        "\n",
        "        if jitter_param is not None:\n",
        "            self.transforms.append(ColorJitter(**jitter_param))\n",
        "\n",
        "    def __call__(self, clip):\n",
        "        for t in self.transforms:\n",
        "            clip = t(clip)\n",
        "        return clip"
      ],
      "metadata": {
        "id": "ADl_9hdXrpoe"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from skimage import io, img_as_float32\n",
        "from skimage.color import gray2rgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imageio import mimread\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from augmentation import AllAugmentationTransform\n",
        "import glob\n",
        "\n",
        "\n",
        "def read_video(name, frame_shape):\n",
        "    \"\"\"\n",
        "    Read video which can be:\n",
        "      - an image of concatenated frames\n",
        "      - '.mp4' and'.gif'\n",
        "      - folder with videos\n",
        "    \"\"\"\n",
        "\n",
        "    if os.path.isdir(name):\n",
        "        frames = sorted(os.listdir(name))\n",
        "        num_frames = len(frames)\n",
        "        video_array = np.array(\n",
        "            [img_as_float32(io.imread(os.path.join(name, frames[idx]))) for idx in range(num_frames)])\n",
        "    elif name.lower().endswith('.png') or name.lower().endswith('.jpg'):\n",
        "        image = io.imread(name)\n",
        "\n",
        "        if len(image.shape) == 2 or image.shape[2] == 1:\n",
        "            image = gray2rgb(image)\n",
        "\n",
        "        if image.shape[2] == 4:\n",
        "            image = image[..., :3]\n",
        "\n",
        "        image = img_as_float32(image)\n",
        "\n",
        "        video_array = np.moveaxis(image, 1, 0)\n",
        "\n",
        "        video_array = video_array.reshape((-1,) + frame_shape)\n",
        "        video_array = np.moveaxis(video_array, 1, 2)\n",
        "    elif name.lower().endswith('.gif') or name.lower().endswith('.mp4') or name.lower().endswith('.mov'):\n",
        "        video = np.array(mimread(name))\n",
        "        if len(video.shape) == 3:\n",
        "            video = np.array([gray2rgb(frame) for frame in video])\n",
        "        if video.shape[-1] == 4:\n",
        "            video = video[..., :3]\n",
        "        video_array = img_as_float32(video)\n",
        "    else:\n",
        "        raise Exception(\"Unknown file extensions  %s\" % name)\n",
        "\n",
        "    return video_array\n",
        "\n",
        "\n",
        "class FramesDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset of videos, each video can be represented as:\n",
        "      - an image of concatenated frames\n",
        "      - '.mp4' or '.gif'\n",
        "      - folder with all frames\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, frame_shape=(256, 256, 3), id_sampling=False, is_train=True,\n",
        "                 random_seed=0, pairs_list=None, augmentation_params=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.videos = os.listdir(root_dir)\n",
        "        self.frame_shape = tuple(frame_shape)\n",
        "        self.pairs_list = pairs_list\n",
        "        self.id_sampling = id_sampling\n",
        "        if os.path.exists(os.path.join(root_dir, 'train')):\n",
        "            assert os.path.exists(os.path.join(root_dir, 'test'))\n",
        "            print(\"Use predefined train-test split.\")\n",
        "            if id_sampling:\n",
        "                train_videos = {os.path.basename(video).split('#')[0] for video in\n",
        "                                os.listdir(os.path.join(root_dir, 'train'))}\n",
        "                train_videos = list(train_videos)\n",
        "            else:\n",
        "                train_videos = os.listdir(os.path.join(root_dir, 'train'))\n",
        "            test_videos = os.listdir(os.path.join(root_dir, 'test'))\n",
        "            self.root_dir = os.path.join(self.root_dir, 'train' if is_train else 'test')\n",
        "        else:\n",
        "            print(\"Use random train-test split.\")\n",
        "            train_videos, test_videos = train_test_split(self.videos, random_state=random_seed, test_size=0.2)\n",
        "\n",
        "        if is_train:\n",
        "            self.videos = train_videos\n",
        "        else:\n",
        "            self.videos = test_videos\n",
        "\n",
        "        self.is_train = is_train\n",
        "\n",
        "        if self.is_train:\n",
        "            self.transform = AllAugmentationTransform(**augmentation_params)\n",
        "        else:\n",
        "            self.transform = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.videos)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.is_train and self.id_sampling:\n",
        "            name = self.videos[idx]\n",
        "            path = np.random.choice(glob.glob(os.path.join(self.root_dir, name + '*.mp4')))\n",
        "        else:\n",
        "            name = self.videos[idx]\n",
        "            path = os.path.join(self.root_dir, name)\n",
        "\n",
        "        video_name = os.path.basename(path)\n",
        "\n",
        "        if self.is_train and os.path.isdir(path):\n",
        "            frames = os.listdir(path)\n",
        "            num_frames = len(frames)\n",
        "            frame_idx = np.sort(np.random.choice(num_frames, replace=True, size=2))\n",
        "            video_array = [img_as_float32(io.imread(os.path.join(path, frames[idx]))) for idx in frame_idx]\n",
        "        else:\n",
        "            video_array = read_video(path, frame_shape=self.frame_shape)\n",
        "            num_frames = len(video_array)\n",
        "            frame_idx = np.sort(np.random.choice(num_frames, replace=True, size=2)) if self.is_train else range(\n",
        "                num_frames)\n",
        "            video_array = video_array[frame_idx]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            video_array = self.transform(video_array)\n",
        "\n",
        "        out = {}\n",
        "        if self.is_train:\n",
        "            source = np.array(video_array[0], dtype='float32')\n",
        "            driving = np.array(video_array[1], dtype='float32')\n",
        "\n",
        "            out['driving'] = driving.transpose((2, 0, 1))\n",
        "            out['source'] = source.transpose((2, 0, 1))\n",
        "        else:\n",
        "            video = np.array(video_array, dtype='float32')\n",
        "            out['video'] = video.transpose((3, 0, 1, 2))\n",
        "\n",
        "        out['name'] = video_name\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class DatasetRepeater(Dataset):\n",
        "    \"\"\"\n",
        "    Pass several times over the same dataset for better i/o performance\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, num_repeats=100):\n",
        "        self.dataset = dataset\n",
        "        self.num_repeats = num_repeats\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_repeats * self.dataset.__len__()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dataset[idx % self.dataset.__len__()]\n",
        "\n",
        "\n",
        "class PairedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset of pairs for animation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, initial_dataset, number_of_pairs, seed=0):\n",
        "        self.initial_dataset = initial_dataset\n",
        "        pairs_list = self.initial_dataset.pairs_list\n",
        "\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        if pairs_list is None:\n",
        "            max_idx = min(number_of_pairs, len(initial_dataset))\n",
        "            nx, ny = max_idx, max_idx\n",
        "            xy = np.mgrid[:nx, :ny].reshape(2, -1).T\n",
        "            number_of_pairs = min(xy.shape[0], number_of_pairs)\n",
        "            self.pairs = xy.take(np.random.choice(xy.shape[0], number_of_pairs, replace=False), axis=0)\n",
        "        else:\n",
        "            videos = self.initial_dataset.videos\n",
        "            name_to_index = {name: index for index, name in enumerate(videos)}\n",
        "            pairs = pd.read_csv(pairs_list)\n",
        "            pairs = pairs[np.logical_and(pairs['source'].isin(videos), pairs['driving'].isin(videos))]\n",
        "\n",
        "            number_of_pairs = min(pairs.shape[0], number_of_pairs)\n",
        "            self.pairs = []\n",
        "            self.start_frames = []\n",
        "            for ind in range(number_of_pairs):\n",
        "                self.pairs.append(\n",
        "                    (name_to_index[pairs['driving'].iloc[ind]], name_to_index[pairs['source'].iloc[ind]]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.pairs[idx]\n",
        "        first = self.initial_dataset[pair[0]]\n",
        "        second = self.initial_dataset[pair[1]]\n",
        "        first = {'driving_' + key: value for key, value in first.items()}\n",
        "        second = {'source_' + key: value for key, value in second.items()}\n",
        "\n",
        "        return {**first, **second}"
      ],
      "metadata": {
        "id": "CqtlrCBnqhZX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import imageio\n",
        "\n",
        "import os\n",
        "from skimage.draw import circle_perimeter as circle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "\n",
        "\n",
        "class Logger:\n",
        "    def __init__(self, log_dir, checkpoint_freq=100, visualizer_params=None, zfill_num=8, log_file_name='log.txt'):\n",
        "\n",
        "        self.loss_list = []\n",
        "        self.cpk_dir = log_dir\n",
        "        self.visualizations_dir = os.path.join(log_dir, 'train-vis')\n",
        "        if not os.path.exists(self.visualizations_dir):\n",
        "            os.makedirs(self.visualizations_dir)\n",
        "        self.log_file = open(os.path.join(log_dir, log_file_name), 'a')\n",
        "        self.zfill_num = zfill_num\n",
        "        self.visualizer = Visualizer(**visualizer_params)\n",
        "        self.checkpoint_freq = checkpoint_freq\n",
        "        self.epoch = 0\n",
        "        self.best_loss = float('inf')\n",
        "        self.names = None\n",
        "\n",
        "    def log_scores(self, loss_names):\n",
        "        loss_mean = np.array(self.loss_list).mean(axis=0)\n",
        "\n",
        "        loss_string = \"; \".join([\"%s - %.5f\" % (name, value) for name, value in zip(loss_names, loss_mean)])\n",
        "        loss_string = str(self.epoch).zfill(self.zfill_num) + \") \" + loss_string\n",
        "\n",
        "        print(loss_string, file=self.log_file)\n",
        "        self.loss_list = []\n",
        "        self.log_file.flush()\n",
        "\n",
        "    def visualize_rec(self, inp, out):\n",
        "        image = self.visualizer.visualize(inp['driving'], inp['source'], out)\n",
        "        imageio.imsave(os.path.join(self.visualizations_dir, \"%s-rec.png\" % str(self.epoch).zfill(self.zfill_num)), image)\n",
        "\n",
        "    def save_cpk(self, emergent=False):\n",
        "        cpk = {k: v.state_dict() for k, v in self.models.items()}\n",
        "        cpk['epoch'] = self.epoch\n",
        "        cpk_path = os.path.join(self.cpk_dir, '%s-checkpoint.pth.tar' % str(self.epoch).zfill(self.zfill_num))\n",
        "        if not (os.path.exists(cpk_path) and emergent):\n",
        "            torch.save(cpk, cpk_path)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_cpk(checkpoint_path, generator=None, discriminator=None, kp_detector=None,\n",
        "                 optimizer_generator=None, optimizer_discriminator=None, optimizer_kp_detector=None):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        if generator is not None:\n",
        "            generator.load_state_dict(checkpoint['generator'])\n",
        "        if kp_detector is not None:\n",
        "            kp_detector.load_state_dict(checkpoint['kp_detector'])\n",
        "        if discriminator is not None:\n",
        "            try:\n",
        "               discriminator.load_state_dict(checkpoint['discriminator'])\n",
        "            except:\n",
        "               print ('No discriminator in the state-dict. Dicriminator will be randomly initialized')\n",
        "        if optimizer_generator is not None:\n",
        "            optimizer_generator.load_state_dict(checkpoint['optimizer_generator'])\n",
        "        if optimizer_discriminator is not None:\n",
        "            try:\n",
        "                optimizer_discriminator.load_state_dict(checkpoint['optimizer_discriminator'])\n",
        "            except RuntimeError as e:\n",
        "                print ('No discriminator optimizer in the state-dict. Optimizer will be not initialized')\n",
        "        if optimizer_kp_detector is not None:\n",
        "            optimizer_kp_detector.load_state_dict(checkpoint['optimizer_kp_detector'])\n",
        "\n",
        "        return checkpoint['epoch']\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        if 'models' in self.__dict__:\n",
        "            self.save_cpk()\n",
        "        self.log_file.close()\n",
        "\n",
        "    def log_iter(self, losses):\n",
        "        losses = collections.OrderedDict(losses.items())\n",
        "        if self.names is None:\n",
        "            self.names = list(losses.keys())\n",
        "        self.loss_list.append(list(losses.values()))\n",
        "\n",
        "    def log_epoch(self, epoch, models, inp, out):\n",
        "        self.epoch = epoch\n",
        "        self.models = models\n",
        "        if (self.epoch + 1) % self.checkpoint_freq == 0:\n",
        "            self.save_cpk()\n",
        "        self.log_scores(self.names)\n",
        "        self.visualize_rec(inp, out)\n",
        "\n",
        "\n",
        "class Visualizer:\n",
        "    def __init__(self, kp_size=5, draw_border=False, colormap='gist_rainbow'):\n",
        "        self.kp_size = kp_size\n",
        "        self.draw_border = draw_border\n",
        "        self.colormap = plt.get_cmap(colormap)\n",
        "\n",
        "    def draw_image_with_kp(self, image, kp_array):\n",
        "        image = np.copy(image)\n",
        "        spatial_size = np.array(image.shape[:2][::-1])[np.newaxis]\n",
        "        kp_array = spatial_size * (kp_array + 1) / 2\n",
        "        num_kp = kp_array.shape[0]\n",
        "        for kp_ind, kp in enumerate(kp_array):\n",
        "            rr, cc = circle(kp[1], kp[0], self.kp_size, shape=image.shape[:2])\n",
        "            image[rr, cc] = np.array(self.colormap(kp_ind / num_kp))[:3]\n",
        "        return image\n",
        "\n",
        "    def create_image_column_with_kp(self, images, kp):\n",
        "        image_array = np.array([self.draw_image_with_kp(v, k) for v, k in zip(images, kp)])\n",
        "        return self.create_image_column(image_array)\n",
        "\n",
        "    def create_image_column(self, images):\n",
        "        if self.draw_border:\n",
        "            images = np.copy(images)\n",
        "            images[:, :, [0, -1]] = (1, 1, 1)\n",
        "            images[:, :, [0, -1]] = (1, 1, 1)\n",
        "        return np.concatenate(list(images), axis=0)\n",
        "\n",
        "    def create_image_grid(self, *args):\n",
        "        out = []\n",
        "        for arg in args:\n",
        "            if type(arg) == tuple:\n",
        "                out.append(self.create_image_column_with_kp(arg[0], arg[1]))\n",
        "            else:\n",
        "                out.append(self.create_image_column(arg))\n",
        "        return np.concatenate(out, axis=1)\n",
        "\n",
        "    def visualize(self, driving, source, out):\n",
        "        images = []\n",
        "\n",
        "        # Source image with keypoints\n",
        "        source = source.data.cpu()\n",
        "        kp_source = out['kp_source']['value'].data.cpu().numpy()\n",
        "        source = np.transpose(source, [0, 2, 3, 1])\n",
        "        images.append((source, kp_source))\n",
        "\n",
        "        # Equivariance visualization\n",
        "        if 'transformed_frame' in out:\n",
        "            transformed = out['transformed_frame'].data.cpu().numpy()\n",
        "            transformed = np.transpose(transformed, [0, 2, 3, 1])\n",
        "            transformed_kp = out['transformed_kp']['value'].data.cpu().numpy()\n",
        "            images.append((transformed, transformed_kp))\n",
        "\n",
        "        # Driving image with keypoints\n",
        "        kp_driving = out['kp_driving']['value'].data.cpu().numpy()\n",
        "        driving = driving.data.cpu().numpy()\n",
        "        driving = np.transpose(driving, [0, 2, 3, 1])\n",
        "        images.append((driving, kp_driving))\n",
        "\n",
        "        # Deformed image\n",
        "        if 'deformed' in out:\n",
        "            deformed = out['deformed'].data.cpu().numpy()\n",
        "            deformed = np.transpose(deformed, [0, 2, 3, 1])\n",
        "            images.append(deformed)\n",
        "\n",
        "        # Result with and without keypoints\n",
        "        prediction = out['prediction'].data.cpu().numpy()\n",
        "        prediction = np.transpose(prediction, [0, 2, 3, 1])\n",
        "        if 'kp_norm' in out:\n",
        "            kp_norm = out['kp_norm']['value'].data.cpu().numpy()\n",
        "            images.append((prediction, kp_norm))\n",
        "        images.append(prediction)\n",
        "\n",
        "\n",
        "        ## Occlusion map\n",
        "        if 'occlusion_map' in out:\n",
        "            occlusion_map = out['occlusion_map'].data.cpu().repeat(1, 3, 1, 1)\n",
        "            occlusion_map = F.interpolate(occlusion_map, size=source.shape[1:3]).numpy()\n",
        "            occlusion_map = np.transpose(occlusion_map, [0, 2, 3, 1])\n",
        "            images.append(occlusion_map)\n",
        "\n",
        "        # Deformed images according to each individual transform\n",
        "        if 'sparse_deformed' in out:\n",
        "            full_mask = []\n",
        "            for i in range(out['sparse_deformed'].shape[1]):\n",
        "                image = out['sparse_deformed'][:, i].data.cpu()\n",
        "                image = F.interpolate(image, size=source.shape[1:3])\n",
        "                mask = out['mask'][:, i:(i+1)].data.cpu().repeat(1, 3, 1, 1)\n",
        "                mask = F.interpolate(mask, size=source.shape[1:3])\n",
        "                image = np.transpose(image.numpy(), (0, 2, 3, 1))\n",
        "                mask = np.transpose(mask.numpy(), (0, 2, 3, 1))\n",
        "\n",
        "                if i != 0:\n",
        "                    color = np.array(self.colormap((i - 1) / (out['sparse_deformed'].shape[1] - 1)))[:3]\n",
        "                else:\n",
        "                    color = np.array((0, 0, 0))\n",
        "\n",
        "                color = color.reshape((1, 1, 1, 3))\n",
        "\n",
        "                images.append(image)\n",
        "                if i != 0:\n",
        "                    images.append(mask * color)\n",
        "                else:\n",
        "                    images.append(mask)\n",
        "\n",
        "                full_mask.append(mask * color)\n",
        "\n",
        "            images.append(sum(full_mask))\n",
        "\n",
        "        image = self.create_image_grid(*images)\n",
        "        image = (255 * image).astype(np.uint8)\n",
        "        return image"
      ],
      "metadata": {
        "id": "8Gdqx_NtrYKc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from frames_dataset import PairedDataset\n",
        "from logger import Logger, Visualizer\n",
        "import imageio\n",
        "from scipy.spatial import ConvexHull\n",
        "import numpy as np\n",
        "\n",
        "from sync_batchnorm import DataParallelWithCallback\n",
        "\n",
        "\n",
        "def normalize_kp(kp_source, kp_driving, kp_driving_initial, adapt_movement_scale=False,\n",
        "                 use_relative_movement=False, use_relative_jacobian=False):\n",
        "    if adapt_movement_scale:\n",
        "        source_area = ConvexHull(kp_source['value'][0].data.cpu().numpy()).volume\n",
        "        driving_area = ConvexHull(kp_driving_initial['value'][0].data.cpu().numpy()).volume\n",
        "        adapt_movement_scale = np.sqrt(source_area) / np.sqrt(driving_area)\n",
        "    else:\n",
        "        adapt_movement_scale = 1\n",
        "\n",
        "    kp_new = {k: v for k, v in kp_driving.items()}\n",
        "\n",
        "    if use_relative_movement:\n",
        "        kp_value_diff = (kp_driving['value'] - kp_driving_initial['value'])\n",
        "        kp_value_diff *= adapt_movement_scale\n",
        "        kp_new['value'] = kp_value_diff + kp_source['value']\n",
        "\n",
        "        if use_relative_jacobian:\n",
        "            jacobian_diff = torch.matmul(kp_driving['jacobian'], torch.inverse(kp_driving_initial['jacobian']))\n",
        "            kp_new['jacobian'] = torch.matmul(jacobian_diff, kp_source['jacobian'])\n",
        "\n",
        "    return kp_new\n",
        "\n",
        "\n",
        "def animate(config, generator, kp_detector, checkpoint, log_dir, dataset):\n",
        "    log_dir = os.path.join(log_dir, 'animation')\n",
        "    png_dir = os.path.join(log_dir, 'png')\n",
        "    animate_params = config['animate_params']\n",
        "\n",
        "    dataset = PairedDataset(initial_dataset=dataset, number_of_pairs=animate_params['num_pairs'])\n",
        "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n",
        "\n",
        "    if checkpoint is not None:\n",
        "        Logger.load_cpk(checkpoint, generator=generator, kp_detector=kp_detector)\n",
        "    else:\n",
        "        raise AttributeError(\"Checkpoint should be specified for mode='animate'.\")\n",
        "\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "\n",
        "    if not os.path.exists(png_dir):\n",
        "        os.makedirs(png_dir)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        generator = DataParallelWithCallback(generator)\n",
        "        kp_detector = DataParallelWithCallback(kp_detector)\n",
        "\n",
        "    generator.eval()\n",
        "    kp_detector.eval()\n",
        "\n",
        "    for it, x in tqdm(enumerate(dataloader)):\n",
        "        with torch.no_grad():\n",
        "            predictions = []\n",
        "            visualizations = []\n",
        "\n",
        "            driving_video = x['driving_video']\n",
        "            source_frame = x['source_video'][:, :, 0, :, :]\n",
        "\n",
        "            kp_source = kp_detector(source_frame)\n",
        "            kp_driving_initial = kp_detector(driving_video[:, :, 0])\n",
        "\n",
        "            for frame_idx in range(driving_video.shape[2]):\n",
        "                driving_frame = driving_video[:, :, frame_idx]\n",
        "                kp_driving = kp_detector(driving_frame)\n",
        "                kp_norm = normalize_kp(kp_source=kp_source, kp_driving=kp_driving,\n",
        "                                       kp_driving_initial=kp_driving_initial, **animate_params['normalization_params'])\n",
        "                out = generator(source_frame, kp_source=kp_source, kp_driving=kp_norm)\n",
        "\n",
        "                out['kp_driving'] = kp_driving\n",
        "                out['kp_source'] = kp_source\n",
        "                out['kp_norm'] = kp_norm\n",
        "\n",
        "                del out['sparse_deformed']\n",
        "\n",
        "                predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
        "\n",
        "                visualization = Visualizer(**config['visualizer_params']).visualize(source=source_frame,\n",
        "                                                                                    driving=driving_frame, out=out)\n",
        "                visualization = visualization\n",
        "                visualizations.append(visualization)\n",
        "\n",
        "            predictions = np.concatenate(predictions, axis=1)\n",
        "            result_name = \"-\".join([x['driving_name'][0], x['source_name'][0]])\n",
        "            imageio.imsave(os.path.join(png_dir, result_name + '.png'), (255 * predictions).astype(np.uint8))\n",
        "\n",
        "            image_name = result_name + animate_params['format']\n",
        "            imageio.mimsave(os.path.join(log_dir, image_name), visualizations)"
      ],
      "metadata": {
        "id": "NVpVswynwtFc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from logger import Logger, Visualizer\n",
        "import numpy as np\n",
        "import imageio\n",
        "from sync_batchnorm import DataParallelWithCallback\n",
        "\n",
        "\n",
        "def reconstruction(config, generator, kp_detector, checkpoint, log_dir, dataset):\n",
        "    png_dir = os.path.join(log_dir, 'reconstruction/png')\n",
        "    log_dir = os.path.join(log_dir, 'reconstruction')\n",
        "\n",
        "    if checkpoint is not None:\n",
        "        Logger.load_cpk(checkpoint, generator=generator, kp_detector=kp_detector)\n",
        "    else:\n",
        "        raise AttributeError(\"Checkpoint should be specified for mode='reconstruction'.\")\n",
        "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n",
        "\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "\n",
        "    if not os.path.exists(png_dir):\n",
        "        os.makedirs(png_dir)\n",
        "\n",
        "    loss_list = []\n",
        "    if torch.cuda.is_available():\n",
        "        generator = DataParallelWithCallback(generator)\n",
        "        kp_detector = DataParallelWithCallback(kp_detector)\n",
        "\n",
        "    generator.eval()\n",
        "    kp_detector.eval()\n",
        "\n",
        "    for it, x in tqdm(enumerate(dataloader)):\n",
        "        if config['reconstruction_params']['num_videos'] is not None:\n",
        "            if it > config['reconstruction_params']['num_videos']:\n",
        "                break\n",
        "        with torch.no_grad():\n",
        "            predictions = []\n",
        "            visualizations = []\n",
        "            if torch.cuda.is_available():\n",
        "                x['video'] = x['video'].cuda()\n",
        "            kp_source = kp_detector(x['video'][:, :, 0])\n",
        "            for frame_idx in range(x['video'].shape[2]):\n",
        "                source = x['video'][:, :, 0]\n",
        "                driving = x['video'][:, :, frame_idx]\n",
        "                kp_driving = kp_detector(driving)\n",
        "                out = generator(source, kp_source=kp_source, kp_driving=kp_driving)\n",
        "                out['kp_source'] = kp_source\n",
        "                out['kp_driving'] = kp_driving\n",
        "                del out['sparse_deformed']\n",
        "                predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
        "\n",
        "                visualization = Visualizer(**config['visualizer_params']).visualize(source=source,\n",
        "                                                                                    driving=driving, out=out)\n",
        "                visualizations.append(visualization)\n",
        "\n",
        "                loss_list.append(torch.abs(out['prediction'] - driving).mean().cpu().numpy())\n",
        "\n",
        "            predictions = np.concatenate(predictions, axis=1)\n",
        "            imageio.imsave(os.path.join(png_dir, x['name'][0] + '.png'), (255 * predictions).astype(np.uint8))\n",
        "\n",
        "            image_name = x['name'][0] + config['reconstruction_params']['format']\n",
        "            imageio.mimsave(os.path.join(log_dir, image_name), visualizations)\n",
        "\n",
        "    print(\"Reconstruction loss: %s\" % np.mean(loss_list))\n"
      ],
      "metadata": {
        "id": "Mus1ZsOg322z"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import trange\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from logger import Logger\n",
        "from modules.model import GeneratorFullModel, DiscriminatorFullModel\n",
        "\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "\n",
        "from sync_batchnorm import DataParallelWithCallback\n",
        "\n",
        "from frames_dataset import DatasetRepeater\n",
        "\n",
        "\n",
        "def train(config, generator, discriminator, kp_detector, checkpoint, log_dir, dataset, device_ids):\n",
        "    train_params = config['train_params']\n",
        "\n",
        "    optimizer_generator = torch.optim.Adam(generator.parameters(), lr=train_params['lr_generator'], betas=(0.5, 0.999))\n",
        "    optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=train_params['lr_discriminator'], betas=(0.5, 0.999))\n",
        "    optimizer_kp_detector = torch.optim.Adam(kp_detector.parameters(), lr=train_params['lr_kp_detector'], betas=(0.5, 0.999))\n",
        "\n",
        "    if checkpoint is not None:\n",
        "        start_epoch = Logger.load_cpk(checkpoint, generator, discriminator, kp_detector,\n",
        "                                      optimizer_generator, optimizer_discriminator,\n",
        "                                      None if train_params['lr_kp_detector'] == 0 else optimizer_kp_detector)\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "\n",
        "    scheduler_generator = MultiStepLR(optimizer_generator, train_params['epoch_milestones'], gamma=0.1,\n",
        "                                      last_epoch=start_epoch - 1)\n",
        "    scheduler_discriminator = MultiStepLR(optimizer_discriminator, train_params['epoch_milestones'], gamma=0.1,\n",
        "                                          last_epoch=start_epoch - 1)\n",
        "    scheduler_kp_detector = MultiStepLR(optimizer_kp_detector, train_params['epoch_milestones'], gamma=0.1,\n",
        "                                        last_epoch=-1 + start_epoch * (train_params['lr_kp_detector'] != 0))\n",
        "\n",
        "    if 'num_repeats' in train_params or train_params['num_repeats'] != 1:\n",
        "        dataset = DatasetRepeater(dataset, train_params['num_repeats'])\n",
        "    dataloader = DataLoader(dataset, batch_size=train_params['batch_size'], shuffle=True, num_workers=6, drop_last=True)\n",
        "\n",
        "    generator_full = GeneratorFullModel(kp_detector, generator, discriminator, train_params)\n",
        "    discriminator_full = DiscriminatorFullModel(kp_detector, generator, discriminator, train_params)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        generator_full = DataParallelWithCallback(generator_full, device_ids=device_ids)\n",
        "        discriminator_full = DataParallelWithCallback(discriminator_full, device_ids=device_ids)\n",
        "\n",
        "    with Logger(log_dir=log_dir, visualizer_params=config['visualizer_params'], checkpoint_freq=train_params['checkpoint_freq']) as logger:\n",
        "        for epoch in trange(start_epoch, train_params['num_epochs']):\n",
        "            for x in dataloader:\n",
        "                losses_generator, generated = generator_full(x)\n",
        "\n",
        "                loss_values = [val.mean() for val in losses_generator.values()]\n",
        "                loss = sum(loss_values)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer_generator.step()\n",
        "                optimizer_generator.zero_grad()\n",
        "                optimizer_kp_detector.step()\n",
        "                optimizer_kp_detector.zero_grad()\n",
        "\n",
        "                if train_params['loss_weights']['generator_gan'] != 0:\n",
        "                    optimizer_discriminator.zero_grad()\n",
        "                    losses_discriminator = discriminator_full(x, generated)\n",
        "                    loss_values = [val.mean() for val in losses_discriminator.values()]\n",
        "                    loss = sum(loss_values)\n",
        "\n",
        "                    loss.backward()\n",
        "                    optimizer_discriminator.step()\n",
        "                    optimizer_discriminator.zero_grad()\n",
        "                else:\n",
        "                    losses_discriminator = {}\n",
        "\n",
        "                losses_generator.update(losses_discriminator)\n",
        "                losses = {key: value.mean().detach().data.cpu().numpy() for key, value in losses_generator.items()}\n",
        "                logger.log_iter(losses=losses)\n",
        "\n",
        "            scheduler_generator.step()\n",
        "            scheduler_discriminator.step()\n",
        "            scheduler_kp_detector.step()\n",
        "\n",
        "            logger.log_epoch(epoch, {'generator': generator,\n",
        "                                     'discriminator': discriminator,\n",
        "                                     'kp_detector': kp_detector,\n",
        "                                     'optimizer_generator': optimizer_generator,\n",
        "                                     'optimizer_discriminator': optimizer_discriminator,\n",
        "                                     'optimizer_kp_detector': optimizer_kp_detector}, inp=x, out=generated)"
      ],
      "metadata": {
        "id": "PWk7U1B639m-"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ffmpeg-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k6u-_zI-qNX",
        "outputId": "d475eb0b-b06f-4703-d1dd-495176cacde9"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (0.18.3)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import yaml\n",
        "from argparse import ArgumentParser\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import imageio\n",
        "import numpy as np\n",
        "from skimage.transform import resize\n",
        "from skimage import img_as_ubyte\n",
        "import torch\n",
        "from sync_batchnorm import DataParallelWithCallback\n",
        "\n",
        "from modules.generator import OcclusionAwareGenerator\n",
        "from modules.keypoint_detector import KPDetector\n",
        "from animate import normalize_kp\n",
        "\n",
        "import ffmpeg\n",
        "from os.path import splitext\n",
        "from shutil import copyfileobj\n",
        "from tempfile import NamedTemporaryFile\n",
        "\n",
        "if sys.version_info[0] < 3:\n",
        "    raise Exception(\"You must use Python 3 or higher. Recommended version is Python 3.7\")\n",
        "\n",
        "def load_checkpoints(config_path, checkpoint_path, cpu=False):\n",
        "\n",
        "    with open(config_path) as f:\n",
        "        config = yaml.full_load(f)\n",
        "\n",
        "    generator = OcclusionAwareGenerator(**config['model_params']['generator_params'],\n",
        "                                        **config['model_params']['common_params'])\n",
        "    if not cpu:\n",
        "        generator.cuda()\n",
        "\n",
        "    kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n",
        "                             **config['model_params']['common_params'])\n",
        "    if not cpu:\n",
        "        kp_detector.cuda()\n",
        "\n",
        "    if cpu:\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
        "    else:\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "    generator.load_state_dict(checkpoint['generator'])\n",
        "    kp_detector.load_state_dict(checkpoint['kp_detector'])\n",
        "\n",
        "    if not cpu:\n",
        "        generator = DataParallelWithCallback(generator)\n",
        "        kp_detector = DataParallelWithCallback(kp_detector)\n",
        "\n",
        "    generator.eval()\n",
        "    kp_detector.eval()\n",
        "\n",
        "    return generator, kp_detector\n",
        "\n",
        "\n",
        "def make_animation(source_image, driving_video, generator, kp_detector, relative=True, adapt_movement_scale=True, cpu=False):\n",
        "    with torch.no_grad():\n",
        "        predictions = []\n",
        "        source = torch.tensor(source_image[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
        "        if not cpu:\n",
        "            source = source.cuda()\n",
        "        driving = torch.tensor(np.array(driving_video)[np.newaxis].astype(np.float32)).permute(0, 4, 1, 2, 3)\n",
        "        kp_source = kp_detector(source)\n",
        "        kp_driving_initial = kp_detector(driving[:, :, 0])\n",
        "\n",
        "        for frame_idx in tqdm(range(driving.shape[2])):\n",
        "            driving_frame = driving[:, :, frame_idx]\n",
        "            if not cpu:\n",
        "                driving_frame = driving_frame.cuda()\n",
        "            kp_driving = kp_detector(driving_frame)\n",
        "            kp_norm = normalize_kp(kp_source=kp_source, kp_driving=kp_driving,\n",
        "                                   kp_driving_initial=kp_driving_initial, use_relative_movement=relative,\n",
        "                                   use_relative_jacobian=relative, adapt_movement_scale=adapt_movement_scale)\n",
        "            out = generator(source, kp_source=kp_source, kp_driving=kp_norm)\n",
        "\n",
        "            predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
        "    return predictions\n",
        "\n",
        "def find_best_frame(source, driving, cpu=False):\n",
        "    import face_alignment  # type: ignore (local file)\n",
        "    from scipy.spatial import ConvexHull\n",
        "\n",
        "    def normalize_kp(kp):\n",
        "        kp = kp - kp.mean(axis=0, keepdims=True)\n",
        "        area = ConvexHull(kp[:, :2]).volume\n",
        "        area = np.sqrt(area)\n",
        "        kp[:, :2] = kp[:, :2] / area\n",
        "        return kp\n",
        "\n",
        "    fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=True,\n",
        "                                      device='cpu' if cpu else 'cuda')\n",
        "    kp_source = fa.get_landmarks(255 * source)[0]\n",
        "    kp_source = normalize_kp(kp_source)\n",
        "    norm  = float('inf')\n",
        "    frame_num = 0\n",
        "    for i, image in tqdm(enumerate(driving)):\n",
        "        kp_driving = fa.get_landmarks(255 * image)[0]\n",
        "        kp_driving = normalize_kp(kp_driving)\n",
        "        new_norm = (np.abs(kp_source - kp_driving) ** 2).sum()\n",
        "        if new_norm < norm:\n",
        "            norm = new_norm\n",
        "            frame_num = i\n",
        "    return frame_num\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = ArgumentParser()\n",
        "    parser.add_argument(\"--config\", required=True, help=\"path to config\")\n",
        "    parser.add_argument(\"--checkpoint\", default='vox-cpk.pth.tar', help=\"path to checkpoint to restore\")\n",
        "\n",
        "    parser.add_argument(\"--source_image\", default='sup-mat/source.png', help=\"path to source image\")\n",
        "    parser.add_argument(\"--driving_video\", default='sup-mat/driving.mp4', help=\"path to driving video\")\n",
        "    parser.add_argument(\"--result_video\", default='result.mp4', help=\"path to output\")\n",
        "\n",
        "    parser.add_argument(\"--relative\", dest=\"relative\", action=\"store_true\", help=\"use relative or absolute keypoint coordinates\")\n",
        "    parser.add_argument(\"--adapt_scale\", dest=\"adapt_scale\", action=\"store_true\", help=\"adapt movement scale based on convex hull of keypoints\")\n",
        "\n",
        "    parser.add_argument(\"--find_best_frame\", dest=\"find_best_frame\", action=\"store_true\",\n",
        "                        help=\"Generate from the frame that is the most alligned with source. (Only for faces, requires face_aligment lib)\")\n",
        "\n",
        "    parser.add_argument(\"--best_frame\", dest=\"best_frame\", type=int, default=None, help=\"Set frame to start from.\")\n",
        "\n",
        "    parser.add_argument(\"--cpu\", dest=\"cpu\", action=\"store_true\", help=\"cpu mode.\")\n",
        "\n",
        "    parser.add_argument(\"--audio\", dest=\"audio\", action=\"store_true\", help=\"copy audio to output from the driving video\" )\n",
        "\n",
        "    parser.set_defaults(relative=False)\n",
        "    parser.set_defaults(adapt_scale=False)\n",
        "    parser.set_defaults(audio_on=False)\n",
        "\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "    source_image = imageio.imread(opt.source_image)\n",
        "    reader = imageio.get_reader(opt.driving_video)\n",
        "    fps = reader.get_meta_data()['fps']\n",
        "    driving_video = []\n",
        "    try:\n",
        "        for im in reader:\n",
        "            driving_video.append(im)\n",
        "    except RuntimeError:\n",
        "        pass\n",
        "    reader.close()\n",
        "\n",
        "    source_image = resize(source_image, (256, 256))[..., :3]\n",
        "    driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]\n",
        "    generator, kp_detector = load_checkpoints(config_path=opt.config, checkpoint_path=opt.checkpoint, cpu=opt.cpu)\n",
        "\n",
        "    if opt.find_best_frame or opt.best_frame is not None:\n",
        "        i = opt.best_frame if opt.best_frame is not None else find_best_frame(source_image, driving_video, cpu=opt.cpu)\n",
        "        print(\"Best frame: \" + str(i))\n",
        "        driving_forward = driving_video[i:]\n",
        "        driving_backward = driving_video[:(i+1)][::-1]\n",
        "        predictions_forward = make_animation(source_image, driving_forward, generator, kp_detector, relative=opt.relative, adapt_movement_scale=opt.adapt_scale, cpu=opt.cpu)\n",
        "        predictions_backward = make_animation(source_image, driving_backward, generator, kp_detector, relative=opt.relative, adapt_movement_scale=opt.adapt_scale, cpu=opt.cpu)\n",
        "        predictions = predictions_backward[::-1] + predictions_forward[1:]\n",
        "    else:\n",
        "        predictions = make_animation(source_image, driving_video, generator, kp_detector, relative=opt.relative, adapt_movement_scale=opt.adapt_scale, cpu=opt.cpu)\n",
        "    imageio.mimsave(opt.result_video, [img_as_ubyte(frame) for frame in predictions], fps=fps)\n",
        "\n",
        "    if opt.audio:\n",
        "        try:\n",
        "            with NamedTemporaryFile(suffix=splitext(opt.result_video)[1]) as output:\n",
        "                ffmpeg.output(ffmpeg.input(opt.result_video).video, ffmpeg.input(opt.driving_video).audio, output.name, c='copy').run()\n",
        "                with open(opt.result_video, 'wb') as result:\n",
        "                    copyfileobj(output, result)\n",
        "        except ffmpeg.Error:\n",
        "            print(\"Failed to copy audio: the driving video may have no audio track or the audio format is invalid.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "RfhDqGV88qIr",
        "outputId": "5a57ee4b-9b50-4fc4-f705-b4aaafec966b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] --config CONFIG [--checkpoint CHECKPOINT]\n",
            "                                [--source_image SOURCE_IMAGE] [--driving_video DRIVING_VIDEO]\n",
            "                                [--result_video RESULT_VIDEO] [--relative] [--adapt_scale]\n",
            "                                [--find_best_frame] [--best_frame BEST_FRAME] [--cpu] [--audio]\n",
            "colab_kernel_launcher.py: error: the following arguments are required: --config\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a5QEP4hN-Yix"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}